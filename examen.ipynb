{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from lxml import html\n",
    "import requests\n",
    "import codecs\n",
    "\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_web_principal = 'https://dreguera.github.io/'\n",
    "codigo_text_página = '//span[@id =\"author\"]/text()'\n",
    "\n",
    "page = requests.get(link_web_principal)\n",
    "tree = html.fromstring(page.content)\n",
    "emisores = tree.xpath(codigo_text_página)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dani Reguera dreguera@mondragon.edu',\n",
       " 'Carlos Cernuda ccernuda@mondragon.edu',\n",
       " 'Dani Reguera dreguera@mondragon.edu',\n",
       " 'Carlos Cernuda ccernuda@mondragon.edu',\n",
       " 'Sara Segura spsegura@mondragon.edu',\n",
       " 'Aitor Bediaga abediaga@mondragon.edu',\n",
       " 'Sara Segura spsegura@mondragon.edu',\n",
       " 'Carlos Cernuda ccernuda@mondragon.edu']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emisores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_web_principal = 'https://dreguera.github.io/'\n",
    "codigo_text_página = '//div[@id =\"content\"]/text()'\n",
    "\n",
    "page = requests.get(link_web_principal)\n",
    "tree = html.fromstring(page.content)\n",
    "mensajes = tree.xpath(codigo_text_página)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras2_tokenizado = [[w.lower() for w in word_tokenize(text)] \n",
    "#            for text in emisores]\n",
    "#lista_palabras2 = palabras2_tokenizado[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras2_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de=\"Dani\"\n",
    "#result=[]\n",
    "#for m in emisores:\n",
    "#        a = m.split(' ')[0]\n",
    "#        if Levenshtein.distance(a ,de)==0:\n",
    "#\n",
    "#result.append(mensajes[0])\n",
    "#print(\"El número de mensajes escritos por Dani, son:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando la librería gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apartado a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_1 = mensajes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_2 = mensajes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs=[]\n",
    "file_docs2=[]\n",
    "tokens = sent_tokenize(mensaje_1)\n",
    "for line in tokens:\n",
    "    file_docs.append(line)\n",
    "    \n",
    "tokens = sent_tokenize(mensaje_2)\n",
    "for line in tokens:\n",
    "    file_docs2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'?': 0, 'clustering': 1, 'densidad': 2, 'habeis': 3, 'meanshift': 4, 'para': 5, 'por': 6, 'visto': 7, 'buenos': 8, 'da': 9, 'datos': 10, 'descartar': 11, 'me': 12, 'muy': 13, 'numã©ricos': 14, 'outliers': 15, 'resultados': 16, 'suelo': 17, 'utilizar': 18, 'y': 19, 'yo': 20}\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['?', 0.38], ['clustering', 0.38], ['densidad', 0.38], ['habeis', 0.38], ['meanshift', 0.38], ['por', 0.38], ['visto', 0.38]]\n",
      "[['buenos', 0.28], ['da', 0.28], ['datos', 0.28], ['descartar', 0.28], ['me', 0.28], ['muy', 0.28], ['numã©ricos', 0.28], ['outliers', 0.28], ['resultados', 0.28], ['suelo', 0.28], ['utilizar', 0.28], ['y', 0.28], ['yo', 0.28]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file_docs2:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.43643576 0.16012815]\n"
     ]
    }
   ],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59656394\n"
     ]
    }
   ],
   "source": [
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity percentage: 29.82819676399231 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}',\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apartado b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_3 = mensajes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_4= mensajes[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs=[]\n",
    "file_docs2=[]\n",
    "tokens = sent_tokenize(mensaje_3)\n",
    "for line in tokens:\n",
    "    file_docs.append(line)\n",
    "    \n",
    "tokens = sent_tokenize(mensaje_4)\n",
    "for line in tokens:\n",
    "    file_docs2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 0.22], ['centrado', 0.22], ['clustering', 0.22], ['dani', 0.22], ['de', 0.22], ['densidad', 0.22], ['el', 0.22], ['en', 0.44], ['he', 0.22], ['hola', 0.22], ['la', 0.22], ['me', 0.22], ['no', 0.22], ['parte', 0.22], ['por', 0.22], ['sobre', 0.22], ['supervisado', 0.22], ['todo', 0.22]]\n",
      "[['dbscan', 0.41], ['hdscan', 0.41], ['hemos', 0.41], ['optics', 0.41], ['visto', 0.41], ['y', 0.41]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file_docs2:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.19518001 0.5477226 ]\n"
     ]
    }
   ],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7429026\n"
     ]
    }
   ],
   "source": [
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity percentage: 37.14512884616852 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}',\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apartado c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_5 = mensajes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje_6= mensajes[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs=[]\n",
    "file_docs2=[]\n",
    "tokens = sent_tokenize(mensaje_5)\n",
    "for line in tokens:\n",
    "    file_docs.append(line)\n",
    "    \n",
    "tokens = sent_tokenize(mensaje_6)\n",
    "for line in tokens:\n",
    "    file_docs2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 0.22], ['centrado', 0.22], ['clustering', 0.22], ['dani', 0.22], ['de', 0.22], ['densidad', 0.22], ['el', 0.22], ['en', 0.44], ['he', 0.22], ['hola', 0.22], ['la', 0.22], ['me', 0.22], ['no', 0.22], ['parte', 0.22], ['por', 0.22], ['sobre', 0.22], ['supervisado', 0.22], ['todo', 0.22]]\n",
      "[['dbscan', 0.41], ['hdscan', 0.41], ['hemos', 0.41], ['optics', 0.41], ['visto', 0.41], ['y', 0.41]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file_docs2:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.5819144 0.       ]\n"
     ]
    }
   ],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5819144\n"
     ]
    }
   ],
   "source": [
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity percentage: 29.09572124481201 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}',\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apartado d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He estado corrigiendo los trabajos de Data Science y la parte de aprendizaje no supervisado la han realizado bastante bien, Â¿QuÃ© algoritmos de clustering has dado en clase?',\n",
       " 'Hola Dani, en la parte de no supervisado me he centrado sobre todo en el clustering por densidad. Hemos visto Optics, DBSCAN y HDSCAN',\n",
       " 'MeanShift para clustering por densidad habeis visto? Yo suelo utilizar para datos numÃ©ricos y me da muy buenos resultados para descartar outliers',\n",
       " 'Si, la verdad es que da buenos resultados pero cuando si puedes hacer ajustes manuales de otros parÃ¡metros, OPTICS y DBSCAN son mÃ¡s Ã³ptimos',\n",
       " 'El tema del clutering por densidad me parece super interesante pero no se si tendrÃ\\xada aplicabilidad en el Reto 7',\n",
       " 'Creo que podrÃ\\xadamos traer a una empresa para que les hablara de HDBSCAN',\n",
       " 'Guay! me parece super interesante y puede ser super enriquecedor',\n",
       " 'Perfecto']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit_transform(mensajes)\n",
    "vectors = vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "csim=cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.40567404, 0.03194383, 0.16574839, 0.07377111,\n",
       "        0.13093073, 0.        , 0.        ],\n",
       "       [0.40567404, 1.        , 0.18898224, 0.15689291, 0.34914862,\n",
       "        0.05163978, 0.06030227, 0.        ],\n",
       "       [0.03194383, 0.18898224, 1.        , 0.1111874 , 0.12371791,\n",
       "        0.14638501, 0.05698029, 0.        ],\n",
       "       [0.16574839, 0.15689291, 0.1111874 , 1.        , 0.12838815,\n",
       "        0.15191091, 0.        , 0.        ],\n",
       "       [0.07377111, 0.34914862, 0.12371791, 0.12838815, 1.        ,\n",
       "        0.        , 0.32897585, 0.        ],\n",
       "       [0.13093073, 0.05163978, 0.14638501, 0.15191091, 0.        ,\n",
       "        1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.06030227, 0.05698029, 0.        , 0.32897585,\n",
       "        0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_vectors(vec1,vec2):\n",
    "    vec1 = vec1.reshape(1,-1)\n",
    "    vec2 = vec2.reshape(1,-1)\n",
    "    return cosine_similarity(vec1,vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.405674042269688"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_vectors(vectors[0],vectors[1]) #los que más se parecen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIKETA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'Title', 'Artist', 'Top Genre', 'Year',\n",
       "       'Beats Per Minute (BPM)', 'Energy', 'Danceability', 'Loudness (dB)',\n",
       "       'Liveness', 'Valence', 'Length (Duration)', 'Acousticness',\n",
       "       'Speechiness', 'Popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Spotify-2000.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_features(row):\n",
    "    return row['Artist']+\" \"+row['Top Genre']\n",
    "df[\"combinadas\"] = df.apply(combined_features, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , ..., 0.  , 0.  , 0.5 ],\n",
       "       [0.  , 1.  , 0.  , ..., 0.25, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.25, 0.  , ..., 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.5 , 0.  , 0.  , ..., 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer().fit_transform(df[\"combinadas\"].values)\n",
    "vectors = vectorizer.toarray()\n",
    "csim = cosine_similarity(vectors)\n",
    "csim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_similares = list(enumerate(csim[indice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "canciones_similares_ordenadas =  sorted(canciones_similares, key=lambda x:x[1], reverse=True)\n",
    "canciones_similares_ordenadas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinco_sim = canciones_similares_ordenadas[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(154, 0.5773502691896258),\n",
       " (231, 0.5773502691896258),\n",
       " (251, 0.5773502691896258),\n",
       " (441, 0.5773502691896258),\n",
       " (869, 0.5773502691896258)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cinco_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### los 5 titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Dit Alles\n",
      "Feeling Good\n",
      "Dochters\n",
      "A Boy Named Sue (Live)\n",
      "Storm And Thunder\n"
     ]
    }
   ],
   "source": [
    "def get_title_from_index(Index):\n",
    "    return df[df.Index == Index][\"Title\"].values[0]\n",
    "\n",
    "contador = 0 \n",
    "for cancion in cinco_sim: \n",
    "    print(get_title_from_index(cancion[0] ))\n",
    "    contador += 1\n",
    "    if contador == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### los 5  autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doe Maar\n",
      "Michael Bublé\n",
      "Marco Borsato\n",
      "Johnny Cash\n",
      "Earth & Fire\n"
     ]
    }
   ],
   "source": [
    "def get_author_from_title(Index):\n",
    "    return df[df.Index == Index][\"Artist\"].values[0]\n",
    "\n",
    "contador = 0 \n",
    "for autor in cinco_sim: \n",
    "    print(get_author_from_title(autor[0]))\n",
    "    contador += 1\n",
    "    if contador == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
